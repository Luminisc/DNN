{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mx.gluon.data.vision.datasets.ImageFolderDataset('./mxnet/data/train', flag = 1, transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.zeros((7721, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, label) in enumerate(dataset):\n",
    "    np_data[i][0] = data.asnumpy()[:, :, 0]\n",
    "    np_data[i][1] = data.asnumpy()[:, :, 1]\n",
    "    np_data[i][2] = data.asnumpy()[:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "p = np.random.permutation(np_data.shape[0])\n",
    "np_data = np_data[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7721, 3, 128, 128)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np_data.astype(np.float32, copy = False) / (255.0 / 2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 9\n",
    "image_iter = mx.io.NDArrayIter(np_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandIter(mx.io.DataIter):\n",
    "    def __init__(self, batch_size, ndim):\n",
    "        self.batch_size = batch_size\n",
    "        self.ndim = ndim\n",
    "        self.provide_data = [('rand', (batch_size, ndim, 1, 1))]\n",
    "        self.provide_label = []\n",
    "\n",
    "    def iter_next(self):\n",
    "        return True\n",
    "\n",
    "    def getdata(self):\n",
    "        return [mx.random.normal(0, 1.0, shape = (self.batch_size, self.ndim, 1, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_iter = RandIter(batch_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bias = True\n",
    "fix_gamma = True\n",
    "eps = 1e-5 + 1e-12\n",
    "\n",
    "rand = mx.sym.Variable('rand')\n",
    "\n",
    "g1 = mx.sym.Deconvolution(rand, kernel = (4, 4), num_filter = 1024, no_bias = no_bias)\n",
    "gbn1 = mx.sym.BatchNorm(g1, fix_gamma = fix_gamma, eps = eps)\n",
    "gact1 = mx.sym.Activation(gbn1, act_type = 'relu')\n",
    "\n",
    "g2 = mx.sym.Deconvolution(gact1, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 512, no_bias = no_bias)\n",
    "gbn2 = mx.sym.BatchNorm(g2, fix_gamma = fix_gamma, eps = eps)\n",
    "gact2 = mx.sym.Activation(gbn2, act_type = 'relu')\n",
    "\n",
    "g3 = mx.sym.Deconvolution(gact2, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 256, no_bias = no_bias)\n",
    "gbn3 = mx.sym.BatchNorm(g3, fix_gamma = fix_gamma, eps = eps)\n",
    "gact3 = mx.sym.Activation(gbn3, act_type = 'relu')\n",
    "\n",
    "g4 = mx.sym.Deconvolution(gact3, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 128, no_bias = no_bias)\n",
    "gbn4 = mx.sym.BatchNorm(g4, fix_gamma = fix_gamma, eps = eps)\n",
    "gact4 = mx.sym.Activation(gbn4, act_type = 'relu')\n",
    "\n",
    "g5 = mx.sym.Deconvolution(gact4, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 3, no_bias = no_bias)\n",
    "generatorSymbol = mx.sym.Activation(g5, name = 'gact5', act_type = 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "\n",
    "d1 = mx.sym.Convolution(data, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 128, no_bias = no_bias)\n",
    "dact1 = mx.sym.LeakyReLU(d1, act_type = 'leaky', slope = 0.2)\n",
    "\n",
    "d2 = mx.sym.Convolution(dact1, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 256, no_bias = no_bias)\n",
    "dbn2 = mx.sym.BatchNorm(d2, fix_gamma = fix_gamma, eps = eps)\n",
    "dact2 = mx.sym.LeakyReLU(dbn2, act_type = 'leaky', slope = 0.2)\n",
    "\n",
    "d3 = mx.sym.Convolution(dact2, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 512, no_bias = no_bias)\n",
    "dbn3 = mx.sym.BatchNorm(d3, fix_gamma = fix_gamma, eps = eps)\n",
    "dact3 = mx.sym.LeakyReLU(dbn3, act_type = 'leaky', slope = 0.2)\n",
    "\n",
    "#d4l = mx.sym.Convolution(dact3, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 768, no_bias = no_bias)\n",
    "#dbn4l = mx.sym.BatchNorm(d4l, fix_gamma = fix_gamma, eps = eps)\n",
    "#dact4l = mx.sym.LeakyReLU(dbn4l, act_type = 'leaky', slope = 0.2)\n",
    "\n",
    "d4 = mx.sym.Convolution(dact3, kernel = (4, 4), stride = (2, 2), pad = (1, 1), num_filter = 1024, no_bias = no_bias)\n",
    "dbn4 = mx.sym.BatchNorm(d4, fix_gamma = fix_gamma, eps = eps)\n",
    "dact4 = mx.sym.LeakyReLU(dbn4, act_type = 'leaky', slope = 0.2)\n",
    "\n",
    "d5 = mx.sym.Convolution(dact4, name = 'd5', kernel = (4, 4), num_filter = 1, no_bias = no_bias)\n",
    "d5 = mx.sym.Flatten(d5)\n",
    "\n",
    "label = mx.sym.Variable('label')\n",
    "discriminatorSymbol = mx.sym.LogisticRegressionOutput(data = d5, label = label, name='dloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.02\n",
    "lr = 0.01\n",
    "beta1 = 0.5\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============Generator Module=============\n",
    "generator = mx.mod.Module(symbol = generatorSymbol, data_names = ('rand',), label_names = None, context = ctx)\n",
    "\n",
    "generator.bind(data_shapes = rand_iter.provide_data)\n",
    "\n",
    "generator.init_params(initializer = mx.init.Normal(sigma))\n",
    "\n",
    "generator.init_optimizer(optimizer = 'adam', optimizer_params = {'learning_rate': lr, 'beta1': beta1,})\n",
    "\n",
    "mods = [generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============Discriminator Module=============\n",
    "discriminator = mx.mod.Module(symbol = discriminatorSymbol, data_names = ('data',), label_names = ('label',), context = ctx)\n",
    "\n",
    "discriminator.bind(data_shapes = image_iter.provide_data, \n",
    "                   label_shapes = [('label', (batch_size,))],\n",
    "                   inputs_need_grad=True)\n",
    "\n",
    "discriminator.init_params(initializer = mx.init.Normal(sigma))\n",
    "\n",
    "discriminator.init_optimizer(optimizer = 'adam', optimizer_params = {'learning_rate': lr, 'beta1': beta1,})\n",
    "\n",
    "mods.append(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buf(buf, num_images, img, shape):\n",
    "    width = buf.shape[0]/shape[1]\n",
    "    height = buf.shape[1]/shape[0]\n",
    "    img_width = int(num_images%width)*shape[0]\n",
    "    img_hight = int(num_images/height)*shape[1]\n",
    "    buf[img_hight:img_hight+shape[1], img_width:img_width+shape[0], :] = img\n",
    "\n",
    "def visualize(fake, real):\n",
    "    fake = fake.transpose((0, 2, 3, 1))\n",
    "    fake = np.clip((fake+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)\n",
    "    \n",
    "    real = real.transpose((0, 2, 3, 1))\n",
    "    real = np.clip((real+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    n = np.ceil(np.sqrt(fake.shape[0]))\n",
    "    fbuff = np.zeros((int(n*fake.shape[1]), int(n*fake.shape[2]), int(fake.shape[3])), dtype=np.uint8)\n",
    "    for i, img in enumerate(fake):\n",
    "        fill_buf(fbuff, i, img, fake.shape[1:3])\n",
    "\n",
    "    rbuff = np.zeros((int(n*real.shape[1]), int(n*real.shape[2]), int(real.shape[3])), dtype=np.uint8)\n",
    "    for i, img in enumerate(real):\n",
    "        fill_buf(rbuff, i, img, real.shape[1:3])\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax1.imshow(fbuff)\n",
    "    \n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2.imshow(rbuff)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n"
     ]
    }
   ],
   "source": [
    "# =============fit===============\n",
    "print('fit')\n",
    "for epoch in range(1):\n",
    "    image_iter.reset()\n",
    "    for i, batch in enumerate(image_iter):\n",
    "\n",
    "        rbatch = rand_iter.next()\n",
    "        generator.forward(rbatch, is_train = True)\n",
    "        outG = generator.get_outputs()\n",
    "\n",
    "        label = mx.nd.zeros((batch_size,), ctx = ctx)\n",
    "        discriminator.forward(mx.io.DataBatch(outG, [label]), is_train = True) #died on 128x128\n",
    "        discriminator.backward()\n",
    "        gradD = [[grad.copyto(grad.context) for grad in grads] for grads in discriminator._exec_group.grad_arrays]\n",
    "\n",
    "        label[:] = 1\n",
    "        batch.label = [label]\n",
    "        discriminator.forward(batch, is_train = True)\n",
    "        discriminator.backward()\n",
    "        for gradsr, gradsf in zip(discriminator._exec_group.grad_arrays, gradD):\n",
    "            for gradr, gradf in zip(gradsr, gradsf):\n",
    "                gradr += gradf\n",
    "        discriminator.update()\n",
    "\n",
    "        discriminator.forward(mx.io.DataBatch(outG, [label]), is_train=True)\n",
    "        discriminator.backward()\n",
    "        diffD = discriminator.get_input_grads()\n",
    "        generator.backward(diffD)\n",
    "        generator.update()\n",
    "\n",
    "        i += 1\n",
    "        if i % 50 == 0:\n",
    "            print('epoch:', epoch, 'iter:', i)\n",
    "            print\n",
    "            print(\"   From generator:        From NN:\")\n",
    "\n",
    "            visualize(outG[0].asnumpy(), batch.data[0].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
